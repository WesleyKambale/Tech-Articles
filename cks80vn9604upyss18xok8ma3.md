## Racial Bias in Health-care Algorithms

During the IndabaX Deep Learning Conference in March 2019, held at Springs Hotel — Bugolobi, we discussed at length the racial bias in healthcare algorithms that have been deployed and tested in most developed countries where Artificial Intelligence (AI) is being heavily used in the day-to-day life.


![1_gT89sWutA_dfxDbiK-OiVg.jpeg](https://cdn.hashnode.com/res/hashnode/image/upload/v1628718108476/DOgoQD8VM.jpeg)

“Dissecting racial bias in an algorithm used to manage the health of populations.”, research by Ziad Obermeyer, et al, published in Science on October 24, concluded that the algorithm is widely used to help manage care for about 200 million people in the United States hospitals was less likely to refer black people than white people who were equally sick to programs that aim to improve care for patients with complex medical needs.

My argument, as a developer and someone that’s interested in Artificial Intelligence, is that these algorithms take on the natural human bias from the developers, and also during the time of training these models — because this is when most algorithms pick their parameters.

“We are still using these algorithms called humans that are really biased,” says Rayid Ghani, a computer scientist at Carnegie Mellon University in Pittsburgh, Pennsylvania. “We’ve tested them and known that they’re horrible, but we still use them to make really important decisions every day.”

Let’s take this home, if I woke up today and came up with a healthcare algorithm, my dataset would largely be made up of Ugandans (who are blacks). The hospital where I’m training my model would also determine which parameters this algorithm would take on. At the end of it all, if I don’t capture something that’s large, say, in the Northern region, then my algorithm would be biased in not considering that parameter.

As developers, we have to supply as many parameters and models as we can to make sure that our algorithms are not biased in any way. Otherwise, we will always have our innate human bias follow us even in computer programs.

There is a need to provide a neutral environment in which we can train our models to reduce the bias which comes up from divided, and single-sided environments.